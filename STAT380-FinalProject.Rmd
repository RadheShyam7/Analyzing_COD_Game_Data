---
title: "STAT380-FinalProject"
author: "Aryan Chimnani, Daniel Gao & Radhe Shyam Vatluri"
date: "2024-05-01"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Front Matter
```{r}
remove(list = ls())

#Add libraries as needed
library(tidyverse)
library(readxl)
library(RecordLinkage)
library(e1071)
library(caTools)
library(caret)
library(randomForest)
library(FNN)

#Read in dataset
CODGames_p1 <- read.csv("~/Downloads/CODGames_p1_380.csv")
CODGames_p2 <- read.csv("~/Downloads/CODGames_p2_380.csv")
CODGameModes <- read.csv("~/Downloads/CODGameModes.csv")
CODMaps <- read.csv("~/Downloads/CODMaps.csv")
```
## Task 1 

We will combine both datasets first by binding the rows together to get a full dataset with both the players. To deal with the spelling mistakes, We will create a function that calculates the similarity score between two sets of strings and returns the closest match by taking the highest match score into consideration. This function will be applied to Map1, Map2, and Choice.
```{r}
#combine p1 and p2 datasets
CombinedCOD<-
  CODGames_p1%>%
  bind_rows(CODGames_p2)

find_closest_match <- function(misspelled_name, correct_names) {
  if (misspelled_name == "") {
    return("")
  }
  
  scores <- jarowinkler(misspelled_name, correct_names)
  closest_match <- correct_names[which.max(scores)]
  return(closest_match)
}

CombinedCOD$Map1 <- sapply(CombinedCOD$Map1, find_closest_match, correct_names = CODMaps$Name)
CombinedCOD$Map2 <- sapply(CombinedCOD$Map2, find_closest_match, correct_names = CODMaps$Name)
CombinedCOD$Choice <- sapply(CombinedCOD$Choice, find_closest_match, correct_names = CODMaps$Name)
```

Which maps are the most likely to win the map vote when they are an option?

To answer this question, We will firstly filter out all the matches where the player joined the match after the voting has been completed. Then, We will create a variable to keep track of how the map was selected, i.e., it won the voting or it was a tie vote, and the map was the first option. To do this, we will use regex to extract the number at the beginning of the string(MapVote) and the number at the end of the string(MapVote). Then, We will create a table with each map, how many times it was listed as Map1, listed as Map2, Won, won by tie, won by vote, and finally a Winning Probability column that calculates the probability by dividing win by vote (not including tie votes as the map would be selected based on being the first option but not actually winning the vote) by the sum of map1 options and map2 options. We will finally plot the results.


```{r}
#Dataset with
Voted<-
  CombinedCOD%>%
  filter(Map1!="")

Voted$Tie <- 
  ifelse(gsub("^(\\d+).*$", "\\1", Voted$MapVote) == gsub("^.*?(\\d+\\s?)$","\\1",Voted$MapVote), "Tie", "Voted")

FirstMapOption<-
  Voted%>%
  group_by(Map1)%>%
  summarise(Map1Option=n())

seconMapOption<-
  Voted%>%
  group_by(Map2)%>%
  summarise(Map2Option=n())

Won<-
  Voted%>%
  group_by(Choice)%>%
  summarise(Won=n())

WonByTie<-
  Voted%>%
  group_by(Choice,Tie)%>%
  summarise(N=n())%>%
  pivot_wider(names_from = "Tie",values_from = "N" )

MapWin<-
  FirstMapOption%>%
  inner_join(seconMapOption, by = c("Map1"="Map2"))

MapWin<-
  MapWin%>%
  inner_join(Won,by = c("Map1"="Choice"))

MapWin<-
  MapWin%>%
  inner_join(WonByTie,by = c("Map1"="Choice"))

MapWin<-
  MapWin%>%
  mutate(WinProb=Voted/(Map1Option+Map2Option))%>%
  rename("Map"="Map1")

ggplot(MapWin,aes(x=Map,y=WinProb))+
  geom_col()+
  coord_flip()+
  labs(title="Winning Probability of Each Map",y="Probability")
```

These maps are most likely to win when they are presented as an option

1. Raid

2. Crossroads Strike

3. Nuketown '84

4. Diesel

5. Standoff

and so on...

## Task 2
Generative AI Model used: Claude 3 Sonnet

Prompt Used: Please solve task 2 (Uploaded "STAT 380 Project Information.pdf" for it's reference)

Solution given by the AI: 

For Task 2, I will use the ChatGPT language model (version dated April 2023) as the generative AI tool. Here is the prompt I provided to ChatGPT:

"I have a dataset containing information about online matches from a first-person shooter video game. The data includes columns for 'Map1', 'Map2', 'MapVote', and 'Choice', where 'Map1' and 'Map2' are the two map options presented to players before each match, 'MapVote' records which map received more votes, and 'Choice' is the map that was ultimately selected for that match.
The research question is: Which maps are the most likely to win the map vote when they are an option?
To answer this, I need to calculate the proportion/probability that a given map wins the vote when it was listed as a candidate ('Map1' or 'Map2'). This includes cases where the map won by getting more votes, as well as cases where it was selected as 'Map1' when the vote was tied.

I also need to handle data quality issues like misspelled map names and extra spaces.
Please provide R code to clean the data, calculate the proportions, and visualize the results to answer this research question."
Here is ChatGPT's response:

Load required libraries

library(dplyr)

library(stringr)

Read the data
data <- read.csv("CODGames_p1_380.csv", stringsAsFactors = FALSE)

Note: I further clarified what I had in my front matter section, and it gave the following code. I didn't include the loading libraries and loading the data as that was already done. It then gave the following code below.

```{r}
# Clean map names
CODMaps$Name <- trimws(CODMaps$Name)
CODGames_p1$Map1 <- trimws(CODGames_p1$Map1)
CODGames_p1$Map2 <- trimws(CODGames_p1$Map2)
CODGames_p1$Choice <- trimws(CODGames_p1$Choice)

CODGames_p1$Map1 <- ifelse(CODGames_p1$Map1 %in% CODMaps$Name, CODGames_p1$Map1, NA)
CODGames_p1$Map2 <- ifelse(CODGames_p1$Map2 %in% CODMaps$Name, CODGames_p1$Map2, NA)
CODGames_p1$Choice <- ifelse(CODGames_p1$Choice %in% CODMaps$Name, CODGames_p1$Choice, NA)

# Calculate votes for each map
map_votes <- CODGames_p1 %>%
  filter(!is.na(Map1) & !is.na(Map2)) %>%
  mutate(winner = ifelse(MapVote == Map1, Map1, ifelse(MapVote == Map2, Map2, Map1))) %>%
  group_by(Map1, Map2) %>%
  summarise(total_votes = n(),
            map1_wins = sum(winner == Map1),
            map2_wins = sum(winner == Map2)) %>%
  gather(map, wins, -Map1, -Map2, -total_votes) %>%
  group_by(map) %>%
  summarise(total_candidates = sum(total_votes),
            total_wins = sum(wins),
            tie_wins = sum(total_votes) - sum(wins)) %>%
  mutate(win_probability = (total_wins + tie_wins) / total_candidates)

# Visualize results
ggplot(map_votes, aes(x = reorder(map, win_probability), y = win_probability, fill = map)) +
  geom_col() +
  coord_flip() +
  labs(x = "Map", y = "Probability of winning vote", title = "Map vote win probability") +
  geom_text(aes(label = scales::percent(win_probability, accuracy = 0.1)), hjust = -0.1, vjust = 0.5)
```

## Analysis of the answer

First off, you can clearly see the hallucination of the LLM. It provided an answer as if it was ChatGPT (even though it is not) and gave a random version, April 2023, just because it was asked to provide a version number in the question.

Similarities:
- Both started off by cleaning the data and combining the datasets
- Both calculated the winning probability and used ggplot to visualize the dataset.

Differences
- We used jarowinkler to find the closest match for misspelled map names whereas the AI did not. It misunderstood the prompt and just trimmed the whitespace and checked it against the list of the correct map names. Even though I did provide a subset of the data later on, it failed to understand the problem being misspelled.
- The AI used the ifelse function to handle the NA values for map names, potentially as it did not have access to the whole dataset. Most LLM websites have a file/line size limit which made it impossible to upload all the resources for it to access.
- The biggest difference is just that the AI got it wrong.The probabilities should showcase the actual chances of each map winning when it is an option, but the AI marked it as 1.00 for all maps and failed to list the map names (instead just having map1_wins and map2_wins)

Strengths:
- The AI produced that code really quickly. Even while being wrong, it is fixable and saves you a lot of time setting up the stucture in the case that you use it for a base.
- The AI made comments and proper indentation which is better from a style perspective
- Our code using the jarowinkler was better than what the AI did (just removing whitespace)

Weaknesses:
- The AI code was wrong and produced the wrong graph as it did not understand the question correctly. 

To conclude, the quality of the answer provided by the AI is not acceptable. It misunderstood the question, providing the probability of map 1 and 2 wins instead each map on the y-axis of the visualization. It did not correct the misspelling in the dataset (like how we used jarowinkler in ours).


## Task 3

**Research Question:** How does the game type affect TotalXP after accounting for the score.

For this research question, we have to prepare the dataset. The preparation for this research question requires the values that start with "HC -" to have that part removed. Additional null values in the TotalXP column needed to be removed as well. A simple mutate function is used and a validator to check if the code ran correctly is involved. 

### Cleaning Game Types
```{r}
#using previously cleaned data: CombinedCOD
CombinedCOD <- CombinedCOD %>%
  filter(!is.na(TotalXP)) %>%
  mutate(GameType = if_else(GameType == "HC - TDM", "TDM", GameType)) %>%
  mutate(GameType = if_else(GameType == "HC - Kill Confirmed", "Kill Confirmed", GameType)) %>%
  mutate(GameType = if_else(GameType == "HC - Hardpoint", "Hardpoint", GameType)) %>%
  mutate(GameType = if_else(GameType == "HC - Domination", "Domination", GameType)) 

#validator for code above
CombinedCOD %>% filter(grepl("^HC", GameType))

```

### Visualization and Table
```{r}
ggplot(data = CombinedCOD,mapping = aes( x = Score, y = TotalXP, color = GameType)) +
  geom_point() +
  geom_smooth(method = lm, se = FALSE)+
  labs(y = "Total Experience Earned",
       title = "Total Experience Earned, Game Type, and Score") 
```

The visualization above attempts to analyze the relationship between the score and the total experience earned. Additionally, the data points are colored to differentiate the game types. The graph above shows the trend line of each game type. We can see that the slope of the trend line differs with each game type and that Domination contains the greatest slope. Additionally, the graph most of the data points are clustered around the range of 0 to 5,000.

```{r}
CombinedCOD %>%
  group_by(GameType) %>%
  summarize(frequency = n(),
            sum = sum(TotalXP),
            min = min(TotalXP),
            max = max(TotalXP),
            mean = mean(TotalXP),
            median = median(TotalXP),
            sd = sd(TotalXP)) %>%
  arrange(desc(sum))
```

The table provides a summary statistic of the total experience earned based on the game type. Viewing the table above, we can see the frequency of the data points for the game types Domination and Kill Confirmed is substantially lower than the game types Hardpoint and Team Deathmatch. The frequency for each game type is Team Deathmatch at 517, Hardpoint at 257, Domination at 14, and Kill Confirmed at 20. Looking at the graph, we can see disparity of the data points is further emphasized.

### Linear Regression Model

The final analysis involves the creation of a linear regression model to further analyze the effects of the game types on total experience earned after accounting for the Score. Before building the model, further data preparation is required. When building a linear model, we created indicators for the game types using TDM as the base case. While testing, we found that using other values as the base case resulted in the same residual standard error, multiple r-squared, and adjusted r-squared. We decided to use TDM as the base case since TDM had the highest frequency leading to a more stable mode.

```{r}

#Create indicators
CombinedCOD <- 
  CombinedCOD%>%
  mutate(Domination = ifelse(GameType == "Domination",1,0),
         KillConfirmed = ifelse(GameType == "Kill Confirmed",1,0),
         Hardpoint = ifelse(GameType == "Hardpoint",1,0))

model <- lm(TotalXP ~ Score + Hardpoint + KillConfirmed + Domination, data = CombinedCOD)
summary(model)


```

**Final Analysis**

The estimated regression equation is given by:

$$\hat{y}_i = 6502.1548 + 2.7052 x_{i,Score} + 2692.1979 x_{i,Hardpoint} -1132.7362 x_{i,Kill Confirmed} + 2570.4367 x_{i,Domination}$$

From the estimated regression, the y-intercept states that the average total experience earned in the game type Team Deathmatch(TDM) is **6,502.15** when all the other x terms are 0. For the variable Score, as Score increases by 1, we expect the total experience earned to increase by **2.71**. For the other game types, as the game types Hardpoint, Kill Confirmed, and Domination increase by 1 count, they expect the total experience earned to increase by **2,692.20, -1,132.74, and 2,570.44**, respectively. For each of the expectation, we have to assume the model does not change.

## Task 4

We will be using 3 methods - Random Forset, k-nearest neighbors (kNN), and Naive Bayes.

Random Forest: Random forest is a commonly-used machine learning algorithm that combines the output of multiple decision trees to reach a single result.

kNN: The k-nearest neighbors (KNN) algorithm is a non-parametric, supervised learning classifier, which uses proximity to make classifications or predictions about the grouping of an individual data point.

Naïve Bayes: It seeks to model the distribution of inputs of a given class or category. It does not learn which features are most important to differentiate between classes.

We will create a variable to track the match outcome (win or loss). Draw matches and matches with missing/NA results will be excluded from the analysis due to their low occurrence frequency, which could lead to class imbalance and potential bias in the predictive models. Additionally, draw matches may not be as relevant or meaningful for the specific goal of predicting outright winners and losers in a competitive setting. Removing these instances will ensure a balanced dataset focused on the primary objective of accurately classifying wins and losses.

We will fairly the results by comparing the accuracy of all three methods.

#### Data Preparation
```{r}
#Data Preparation
score_split <- strsplit(CombinedCOD$Result, "-")
CombinedCOD$first_score <- as.numeric(sapply(score_split, function(x) x[1]))
CombinedCOD$second_score <- as.numeric(sapply(score_split, function(x) x[2]))
CombinedCOD$Won <- ifelse(CombinedCOD$first_score > CombinedCOD$second_score, 1, 0)

# remove Draw matches due to their low occurrence frequency, which could lead to class imbalance and potential bias 
CombinedCOD<-
  CombinedCOD%>%
  filter(first_score != second_score)

#scaling
xvars <- c("Score", "Eliminations", "Deaths", "Damage", "TotalXP")
CombinedCOD[ , xvars] <- scale(CombinedCOD[ , xvars], center = TRUE, scale = TRUE)

set.seed(123)
trainInd <- sample(1:nrow(CombinedCOD), floor(0.80*nrow(CombinedCOD)))
set.seed(NULL)

Train <- CombinedCOD[trainInd,]
Validation <- CombinedCOD[-trainInd,]
```

Research Question: Which of the following machine learning models: Random Forest, k-Nearest Neighbors (kNN), or Naive Bayes Classifier, can most accurately predict match outcome (win or loss) for a player based on their individual performance metrics, such as number of eliminations, deaths, score, damage dealt, and total experience points (XP) earned?

#### Naive Bayes:
```{r}
classifier <- naiveBayes(Won ~ Eliminations + Deaths+ Score+Damage+TotalXP, data = Train)
y_pred <- predict(classifier, newdata = Validation)
cm <- table(Validation$Won, y_pred)
cm

confusionMatrix(cm)
```

### Random Forest
```{r}
# Convet won to factor
Train$Won <- as.factor(Train$Won)
Validation$Won <- as.factor(Validation$Won)

# Implementing the Random Forest model
set.seed(123)
rfClassifier <- randomForest(Won ~ Eliminations + Deaths + Score + Damage + TotalXP, data = Train,
                             ntree = 500,
                             mtry = 3,
                             importance = FALSE)
set.seed(NULL)

# Predicting the Validation set results
rf_y_pred <- predict(rfClassifier, newdata = Validation)

# Creating the confusion matrix
rfCM <- table(Validation$Won, rf_y_pred)
rfCM

confusionMatrix(rfCM)
```

#### KNN Classification
```{r}
maxK <-50
acc_vec <- rep(NA, maxK)

#Build kNN classification model
for(i in 1:maxK){
  knn_res <- knn(train = Train[ , xvars, drop = FALSE],
               test = Validation[ , xvars, drop = FALSE],
               cl = Train$Won,
               k = i)

  
  #This could be skipped, but I have included it to be consistent with notes
  Validation <- Validation %>% mutate(pred_Won = knn_res)
  
  #Find and store accuracy
  acc_vec[i] <- mean(Validation$pred_Won == Validation$Won)
}
#Create Plot
temp_df <- data.frame(k = 1:maxK, accuracy = acc_vec)

ggplot(data = temp_df, mapping = aes(x = k, y = accuracy)) +
  geom_line() +
  labs(x = "Number of Nearest Neighbors",
       y = "Accuracy") +
  geom_point(data = temp_df[which.max(acc_vec), ], 
             color = "red", 
             size = 3,
             shape = 1)

#The K value
which.max(acc_vec)

#Highest Accuracy
max(acc_vec)
```
Our findings indicate that kNN is the best method to answer our research question as it had the highest accuracy of 0.689441 compared to Random Forest's 0.646 and Naive Bayes' 0.677.